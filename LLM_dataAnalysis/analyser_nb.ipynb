{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis using LLM models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading the essential packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-nomic\n",
      "  Downloading langchain_nomic-0.0.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-nomic) (0.1.46)\n",
      "Collecting nomic<4.0.0,>=3.0.12 (from langchain-nomic)\n",
      "  Downloading nomic-3.0.25.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-nomic) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-nomic) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-nomic) (0.1.51)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-nomic) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-nomic) (2.7.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-nomic) (8.2.3)\n",
      "Requirement already satisfied: click in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (8.1.7)\n",
      "Collecting jsonlines (from nomic<4.0.0,>=3.0.12->langchain-nomic)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting loguru (from nomic<4.0.0,>=3.0.12->langchain-nomic)\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: rich in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (13.7.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (2.31.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (1.5.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (4.66.2)\n",
      "Collecting pyarrow (from nomic<4.0.0,>=3.0.12->langchain-nomic)\n",
      "  Downloading pyarrow-16.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pillow in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (10.3.0)\n",
      "Collecting pyjwt (from nomic<4.0.0,>=3.0.12->langchain-nomic)\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1->langchain-nomic) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1->langchain-nomic) (3.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1->langchain-nomic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1->langchain-nomic) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1->langchain-nomic) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests->nomic<4.0.0,>=3.0.12->langchain-nomic) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests->nomic<4.0.0,>=3.0.12->langchain-nomic) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests->nomic<4.0.0,>=3.0.12->langchain-nomic) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests->nomic<4.0.0,>=3.0.12->langchain-nomic) (2024.2.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from jsonlines->nomic<4.0.0,>=3.0.12->langchain-nomic) (23.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from pandas->nomic<4.0.0,>=3.0.12->langchain-nomic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from pandas->nomic<4.0.0,>=3.0.12->langchain-nomic) (2024.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from rich->nomic<4.0.0,>=3.0.12->langchain-nomic) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from rich->nomic<4.0.0,>=3.0.12->langchain-nomic) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->nomic<4.0.0,>=3.0.12->langchain-nomic) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->nomic<4.0.0,>=3.0.12->langchain-nomic) (1.16.0)\n",
      "Downloading langchain_nomic-0.0.2-py3-none-any.whl (3.4 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: nomic\n",
      "  Building wheel for nomic (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nomic: filename=nomic-3.0.25-py3-none-any.whl size=44318 sha256=96e6f6f34e93f0ce8721a07fbc9ddd21321b66f0c0545f1721a41d8bd7a13924\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/48/3e/81/caa78ba4afad07dd8e51f8e54384279ff8f590a17c403a1e2d\n",
      "Successfully built nomic\n",
      "Installing collected packages: pyjwt, pyarrow, loguru, jsonlines, nomic, langchain-nomic\n",
      "Successfully installed jsonlines-4.0.0 langchain-nomic-0.0.2 loguru-0.7.2 nomic-3.0.25 pyarrow-16.0.0 pyjwt-2.8.0\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2024.4.28-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Downloading tiktoken-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.4.28-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.2/785.2 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.4.28 tiktoken-0.6.0\n",
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.15-py3-none-any.whl.metadata (621 bytes)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchainhub) (2.31.0)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
      "Downloading langchainhub-0.1.15-py3-none-any.whl (4.6 kB)\n",
      "Downloading types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: types-requests, langchainhub\n",
      "Successfully installed langchainhub-0.1.15 types-requests-2.31.0.20240406\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.0.39-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: langchain-core<0.2.0,>=0.1.46 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langgraph) (0.1.46)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langgraph) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langgraph) (0.1.51)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langgraph) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langgraph) (2.7.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langgraph) (8.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langgraph) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langgraph) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langgraph) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langgraph) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langgraph) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langgraph) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langgraph) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langgraph) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langgraph) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/data_analyser_ai/.env/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langgraph) (2024.2.2)\n",
      "Downloading langgraph-0.0.39-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langgraph\n",
      "Successfully installed langgraph-0.0.39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "# !{sys.executable} -m pip install langchain\n",
    "# !{sys.executable} -m pip install langchain_community\n",
    "# !{sys.executable} -m pip install pandasai\n",
    "# !{sys.executable} -m pip install ollama\n",
    "# !{sys.executable} -m pip install chromadb\n",
    "# !{sys.executable} -m pip install pysqlite3-binary\n",
    "!{sys.executable} -m pip install -U langchain-nomic \n",
    "!{sys.executable} -m pip install -U tiktoken \n",
    "!{sys.executable} -m pip install -U langchainhub \n",
    "!{sys.executable} -m pip install -U langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "from pandasai import SmartDataframe\n",
    "from pandasai import Agent\n",
    "# Instantiate a LLM\n",
    "from pandasai.llm import OpenAI\n",
    "from pandasai.llm.local_llm import LocalLLM\n",
    "from langchain_community.llms import Ollama\n",
    "import ollama\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import dataframe\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>172954319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DR Congo</td>\n",
       "      <td>102262808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>128455567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>339996563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>277534122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>85816199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Iran</td>\n",
       "      <td>89172767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China</td>\n",
       "      <td>1425671352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>71801279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nigeria</td>\n",
       "      <td>223804632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country  Population\n",
       "10     Bangladesh   172954319\n",
       "16       DR Congo   102262808\n",
       "12         Mexico   128455567\n",
       "0   United States   339996563\n",
       "5       Indonesia   277534122\n",
       "3          Turkey    85816199\n",
       "18           Iran    89172767\n",
       "1           China  1425671352\n",
       "19       Thailand    71801279\n",
       "7         Nigeria   223804632"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data \n",
    "data = pd.read_csv('../data/population.csv')\n",
    "data.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init an llm model\n",
    "# llm = LocalLLM(api_base=\"http://localhost:11434/v1\",model='llama3')\n",
    "gpt4_llm = OpenAI(api_token=\"OPENAIKEY\",model=\"gpt-4\")\n",
    "\n",
    "llm = Ollama(model = 'llama3')\n",
    "sdf = SmartDataframe(data, config={\"llm\":gpt4_llm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Population</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>India</td>\n",
       "      <td>1428627663</td>\n",
       "      <td>country:  India  population: str(1428627663)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country  Population                                           doc\n",
       "8   India  1428627663  country:  India  population: str(1428627663)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.chat('Which country which has max population and what is the value?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_population_country = dfs[0][dfs[0]['Population'] == dfs[0]['Population'].max()]\n",
      "result = {'type': 'dataframe', 'value': max_population_country}\n"
     ]
    }
   ],
   "source": [
    "print(sdf.last_code_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1428627663"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.chat(\"what is the popluation of India? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population_india = None\n",
      "for df in dfs:\n",
      "    if 'India' in df['Country'].values:\n",
      "        population_india = df.loc[df['Country'] == 'India', 'Population'].values[0]\n",
      "        break\n",
      "result = {'type': 'number', 'value': population_india}\n"
     ]
    }
   ],
   "source": [
    "print(sdf.last_code_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'country:  india  population: str(3444444)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_doc (country, population):\n",
    "    return f\"country:  {country}  population: str({population })\"\n",
    "\n",
    "create_doc('india',3444444) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['doc'] = data.apply(lambda x: create_doc(x['Country'],x['Population']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = dataframe.DataFrameLoader(data,page_content_column='doc').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ollama embeddings and vector store\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Ollama LLM function\n",
    "def ollama_llm(question, context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context} , give the answer with respect to context\"\n",
    "    response = ollama.chat(model='llama3', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG chain\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    return ollama_llm(question, formatted_context) , retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "result , evm_docs =rag_chain(\"the population str of country Russia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see what you're getting at!\n",
      "\n",
      "You want me to extract the population figures from the given contexts, right?\n",
      "\n",
      "Alright, let's do that:\n",
      "\n",
      "**Russia**: The population of Russia is approximately **144.44 million** (as per your context).\n",
      "\n",
      "**Japan**: And, according to your context, the population of Japan is approximately **12.32945 million**.\n",
      "\n",
      "Please note that these figures might not be up-to-date or accurate, as they are based on the provided contexts and not actual census data.\n"
     ]
    }
   ],
   "source": [
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='country:  Russia  population: str(144444359)', metadata={'Country': 'Russia', 'Population': 144444359}), Document(page_content='country:  Russia  population: str(144444359)', metadata={'Country': 'Russia', 'Population': 144444359}), Document(page_content='country:  Japan  population: str(123294513)', metadata={'Country': 'Japan', 'Population': 123294513}), Document(page_content='country:  Japan  population: str(123294513)', metadata={'Country': 'Japan', 'Population': 123294513})]\n"
     ]
    }
   ],
   "source": [
    "print(evm_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analyser_env",
   "language": "python",
   "name": "analyser_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
